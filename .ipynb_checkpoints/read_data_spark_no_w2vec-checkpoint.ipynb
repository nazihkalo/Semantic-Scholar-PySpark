{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from HDFS\n",
    "\n",
    "Datasource: Semantic Scholar Open Research Corpus\n",
    "\n",
    "Description: Semantic Scholar's records for research papers published in all fields provided as an easy-to-use JSON archive.\n",
    "\n",
    "### Attribute Definitions\n",
    "1. id  string = S2 generated research paper ID.\n",
    "- title  string = Research paper title.\n",
    "- paperAbstract  string = Extracted abstract of the paper.\n",
    "- entities  list = Extracted list of relevant entities or topics.\n",
    "- s2Url  string = URL to S2 research paper details page.\n",
    "- s2PdfUrl  string = URL to PDF on S2 if available.\n",
    "- pdfUrls  list = URLs related to this PDF scraped from the web.\n",
    "- authors  list = List of authors with an S2 generated author ID and name.\n",
    "- inCitations  list = List of S2 paper IDs which cited this paper.\n",
    "- outCitations  list = List of S2 paper IDs which this paper cited\n",
    "- year  int = Year this paper was published as integer.\n",
    "- venue  string = Extracted publication venue for this paper.\n",
    "- journalName  string = Name of the journal that published this paper.\n",
    "- journalVolume  string = The volume of the journal where this paper was published.\n",
    "- journalPages  string = The pages of the journal where this paper was published.\n",
    "- sources  list = Identifies papers sourced from DBLP or Medline.\n",
    "- doi  string = Digital Object Identifier registered at doi.org.\n",
    "- doiUrl  string = DOI link for registered objects.\n",
    "- pmid  string = Unique identifier used by PubMed.\n",
    "- fieldsOfStudy  list = Zero or more fields of study this paper addresses.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#Graph network imports\n",
    "from graphframes import *\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql.functions import udf #user defined function\n",
    "from pyspark.sql.types import * #Import types == IntegerType, StringType etc.\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create Spark session\n",
    "spark = SparkSession.builder.enableHiveSupport().appName('Final_project_read_write').getOrCreate()\n",
    "\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '15g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '10'), ('spark.cores.max', '10'), ('spark.driver.memory','20g')])\n",
    "\n",
    "#print spark configuration settings\n",
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "Found 10 items\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1592749572 2020-02-11 10:25 big_data_project/unzipped_files/s2-corpus-000\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1592820732 2020-02-11 10:24 big_data_project/unzipped_files/s2-corpus-001\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1595233436 2020-02-11 10:24 big_data_project/unzipped_files/s2-corpus-002\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1597066383 2020-02-11 10:24 big_data_project/unzipped_files/s2-corpus-003\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1601510198 2020-02-11 10:24 big_data_project/unzipped_files/s2-corpus-004\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1593588156 2020-02-11 10:25 big_data_project/unzipped_files/s2-corpus-005\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1596634329 2020-02-11 10:23 big_data_project/unzipped_files/s2-corpus-006\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1596512143 2020-02-11 10:25 big_data_project/unzipped_files/s2-corpus-007\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1595234796 2020-02-11 10:25 big_data_project/unzipped_files/s2-corpus-008\n",
      "-rw-r--r--   3 nazihkalo nazihkalo 1596753944 2020-02-11 10:25 big_data_project/unzipped_files/s2-corpus-009\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls big_data_project/unzipped_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MKL_NUM_THREADS': '1', 'OPENBLAS_NUM_THREADS': '1', 'PYTHONHASHSEED': '0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all 10 json files listed above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.json('big_data_project/unzipped_files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of records/articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9976989"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the schema & summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ids: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- doiUrl: string (nullable = true)\n",
      " |-- entities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fieldsOfStudy: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- journalName: string (nullable = true)\n",
      " |-- journalPages: string (nullable = true)\n",
      " |-- journalVolume: string (nullable = true)\n",
      " |-- outCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- paperAbstract: string (nullable = true)\n",
      " |-- pdfUrls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- s2PdfUrl: string (nullable = true)\n",
      " |-- s2Url: string (nullable = true)\n",
      " |-- sources: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('authors', 'array<struct<ids:array<string>,name:string>>'),\n",
       " ('doi', 'string'),\n",
       " ('doiUrl', 'string'),\n",
       " ('entities', 'array<string>'),\n",
       " ('fieldsOfStudy', 'array<string>'),\n",
       " ('id', 'string'),\n",
       " ('inCitations', 'array<string>'),\n",
       " ('journalName', 'string'),\n",
       " ('journalPages', 'string'),\n",
       " ('journalVolume', 'string'),\n",
       " ('outCitations', 'array<string>'),\n",
       " ('paperAbstract', 'string'),\n",
       " ('pdfUrls', 'array<string>'),\n",
       " ('pmid', 'string'),\n",
       " ('s2PdfUrl', 'string'),\n",
       " ('s2Url', 'string'),\n",
       " ('sources', 'array<string>'),\n",
       " ('title', 'string'),\n",
       " ('venue', 'string'),\n",
       " ('year', 'bigint')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+-------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+--------------------+--------------------+--------+--------+--------------------+---------+--------------------+--------+----+\n",
      "|             authors|                 doi|              doiUrl|entities|      fieldsOfStudy|                  id|         inCitations|journalName|journalPages|journalVolume|        outCitations|       paperAbstract|             pdfUrls|    pmid|s2PdfUrl|               s2Url|  sources|               title|   venue|year|\n",
      "+--------------------+--------------------+--------------------+--------+-------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+--------------------+--------------------+--------+--------+--------------------+---------+--------------------+--------+----+\n",
      "|[[[52602509], Yan...|                    |                    |      []|         [Business]|b9c27ac1bb8b3ec9d...|                  []|           |            |             |                  []|The problem of bu...|                  []|        |        |https://semantics...|       []|Cause Analysis an...|        |2014|\n",
      "|[[[122800702], Sa...|                    |                    |      []|          [History]|91cac7b12800a6238...|                  []|           |            |             |                  []|                    |                  []|        |        |https://semantics...|       []|Clair comme lune ...|        |2016|\n",
      "|[[[1392355377], M...|  10.1111/jocn.12477|https://doi.org/1...|      []|         [Medicine]|412b10a92babf3509...|                  []|           |            |             |                  []|Aims and objectiv...|                  []|        |        |https://semantics...|       []|Unexpected deaths...|        |2013|\n",
      "|[[[47909706], Ya ...|10.1371/journal.p...|https://doi.org/1...|      []|[Medicine, Biology]|0de1cecd2ed49812f...|[8ecaab2a03953fa5...|           |    551 - 60|           10|[cf5cdba6424524ee...|The mitogen-activ...|[http://ftp.ncbi....|26599013|        |https://semantics...|[Medline]|RNA-Seq Analysis ...|PloS one|2015|\n",
      "|[[[47264369], Aws...|10.1016/S0735-109...|https://doi.org/1...|      []|         [Medicine]|1d9fee40f59bf00eb...|                  []|           |            |             |                  []|                    |                  []|        |        |https://semantics...|       []|Temporal Trends a...|        |2012|\n",
      "+--------------------+--------------------+--------------------+--------+-------------------+--------------------+--------------------+-----------+------------+-------------+--------------------+--------------------+--------------------+--------+--------+--------------------+---------+--------------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if any NaN rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9784508"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Null rows =  192481\n",
      "Nulls as pct of total = 1.92924939578464%\n"
     ]
    }
   ],
   "source": [
    "#Calculating total number of NAN rows\n",
    "row_count = spark_df.count()\n",
    "null_row_count = row_count - spark_df.dropna().count()\n",
    "print(\"Total Null rows = \", null_row_count)\n",
    "print(\"Nulls as pct of total = {}%\".format((null_row_count/row_count)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.select('fieldsOfStudy').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|      fieldsOfStudy|  count|\n",
      "+-------------------+-------+\n",
      "|         [Medicine]|1797410|\n",
      "|                 []|1270796|\n",
      "|        [Chemistry]| 647360|\n",
      "| [Computer Science]| 605115|\n",
      "|      [Engineering]| 532338|\n",
      "|          [Biology]| 493352|\n",
      "|          [Physics]| 456051|\n",
      "|[Materials Science]| 427580|\n",
      "|       [Psychology]| 354488|\n",
      "|          [History]| 312121|\n",
      "|        [Sociology]| 299245|\n",
      "|              [Art]| 292479|\n",
      "|      [Mathematics]| 269465|\n",
      "|[Political Science]| 268942|\n",
      "|        [Geography]| 250747|\n",
      "|        [Economics]| 198698|\n",
      "|[Medicine, Biology]| 189729|\n",
      "|         [Business]| 184848|\n",
      "|[Biology, Medicine]| 170081|\n",
      "|          [Geology]| 166235|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupby('fieldsOfStudy').count().sort(\"count\").orderBy([\"count\"], ascending=[0]).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.select('year').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating column that counts the inCitations = how many times this paper was cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf #user defined function\n",
    "from pyspark.sql.types import * #Import types == IntegerType, StringType etc.\n",
    "\n",
    "length = udf(lambda listt: len(listt), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df2 = spark_df.withColumn('inCitations_count', length(spark_df['inCitations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|inCitations_count|         inCitations|\n",
      "+-----------------+--------------------+\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|               13|[8ecaab2a03953fa5...|\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|               22|[ecec08f5ca15e07d...|\n",
      "|                2|[2da3df885d1f032d...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df2.select('inCitations_count', 'inCitations').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating column that counts the outCitations = how many other papers this paper cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|outCitations_count|        outCitations|\n",
      "+------------------+--------------------+\n",
      "|                 0|                  []|\n",
      "|                 0|                  []|\n",
      "|                 0|                  []|\n",
      "|                69|[cf5cdba6424524ee...|\n",
      "|                 0|                  []|\n",
      "|                 0|                  []|\n",
      "|                 0|                  []|\n",
      "|                 0|                  []|\n",
      "|                 0|                  []|\n",
      "|                 0|                  []|\n",
      "+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df2 = spark_df2.withColumn('outCitations_count', length(spark_df2['outCitations']))\n",
    "spark_df2.select('outCitations_count', 'outCitations').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting distinct values in each column\n",
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "distinct_pdf = spark_df2.agg(*(countDistinct(col(c)).alias(c) for c in spark_df2.columns)).collect()\n",
    "\n",
    "distinct_pdf = pd.DataFrame(pd.DataFrame(distinct_pdf, columns = spark_df2.columns).stack(0)).rename({0:'distinct_cnt'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>distinct_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"22\" valign=\"top\">0</th>\n",
       "      <th>entities</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2PdfUrl</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sources</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outCitations_count</th>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inCitations_count</th>\n",
       "      <td>1654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldsOfStudy</th>\n",
       "      <td>2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalName</th>\n",
       "      <td>50554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venue</th>\n",
       "      <td>61054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalVolume</th>\n",
       "      <td>77493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalPages</th>\n",
       "      <td>411597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outCitations</th>\n",
       "      <td>1572215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmid</th>\n",
       "      <td>1692355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdfUrls</th>\n",
       "      <td>1783133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inCitations</th>\n",
       "      <td>2846805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doiUrl</th>\n",
       "      <td>4886396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doi</th>\n",
       "      <td>4886420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paperAbstract</th>\n",
       "      <td>5525030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authors</th>\n",
       "      <td>8375458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>9915513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2Url</th>\n",
       "      <td>9976989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>9976989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      distinct_cnt\n",
       "0 entities                       1\n",
       "  s2PdfUrl                       1\n",
       "  sources                        5\n",
       "  year                          85\n",
       "  outCitations_count           678\n",
       "  inCitations_count           1654\n",
       "  fieldsOfStudy               2180\n",
       "  journalName                50554\n",
       "  venue                      61054\n",
       "  journalVolume              77493\n",
       "  journalPages              411597\n",
       "  outCitations             1572215\n",
       "  pmid                     1692355\n",
       "  pdfUrls                  1783133\n",
       "  inCitations              2846805\n",
       "  doiUrl                   4886396\n",
       "  doi                      4886420\n",
       "  paperAbstract            5525030\n",
       "  authors                  8375458\n",
       "  title                    9915513\n",
       "  s2Url                    9976989\n",
       "  id                       9976989"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_pdf.sort_values('distinct_cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The problem of budgetary overrun in real estate projects has been enterprises attach great importance.In this paper,we analyze the cause of budgetary overrun in real estate projects in view of building materials,equipment investment and construction;secondly,from the construction costs of the standardized management,enterprise system management and other aspects,we super effective countermeasures to control the budgetary overrun in real estate projects;finally,taking an example of a real estate project in Hunan Province,in detail we analyze the causes and control measures of budgetary overrun in real estate projects.The study can be implemented to provide guidance for similar projects.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df2.select('paperAbstract').limit(1).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the abstract & title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Clean text\n",
    "spark_df2 = spark_df2.withColumn('paperAbstract', (lower(regexp_replace('paperAbstract', \"[^a-zA-Z\\\\s]\", \"\"))))\n",
    "spark_df2 = spark_df2.withColumn('title', (lower(regexp_replace('title', \"[^a-zA-Z\\\\s]\", \"\"))))\n",
    "\n",
    "# word_count = udf(lambda string: len(nltk.word_tokenize(string)), IntegerType()) ## OLD SOLUTION \n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "#tokenize words\n",
    "tokenizer = Tokenizer(inputCol=\"paperAbstract\", outputCol=\"abstract_tokens\")\n",
    "spark_df2 = tokenizer.transform(spark_df2)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_tokens\")\n",
    "spark_df2 = tokenizer.transform(spark_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "#remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"abstract_tokens\", outputCol=\"abstract_tokens_filtered\")\n",
    "spark_df2 = remover.transform(spark_df2)\n",
    "\n",
    "#remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"title_tokens\", outputCol=\"title_tokens_filtered\")\n",
    "spark_df2 = remover.transform(spark_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter length word > 3\n",
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
    "\n",
    "spark_df2 = spark_df2.withColumn('abstract_tokens_filtered', filter_length_udf(col('abstract_tokens_filtered')))\n",
    "spark_df2 = spark_df2.withColumn('title_tokens_filtered', filter_length_udf(col('title_tokens_filtered')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmetize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/nazihkalo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizationFunct(x):\n",
    "    finalLem = [lemmatizer.lemmatize(s) for s in x]\n",
    "    return finalLem\n",
    "\n",
    "lemmetize_udf = udf(lambda tokens: lemmatizationFunct(tokens), ArrayType(StringType())) \n",
    "\n",
    "spark_df2 = spark_df2.withColumn('abstract_tokens_filtered_lem', lemmetize_udf(spark_df2['abstract_tokens_filtered']))\n",
    "spark_df2 = spark_df2.withColumn('title_tokens_filtered_lem', lemmetize_udf(spark_df2['title_tokens_filtered']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create wcount columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------+----------------------------+---------------+\n",
      "|title_tokens_filtered_lem|title_wcount|abstract_tokens_filtered_lem|abstract_wcount|\n",
      "+-------------------------+------------+----------------------------+---------------+\n",
      "|     [cause, analysis,...|           9|        [problem, budgeta...|             61|\n",
      "|     [clair, comme, lu...|          14|                          []|              0|\n",
      "|     [unexpected, deat...|          10|        [aim, objective, ...|            197|\n",
      "|     [rnaseq, analysis...|          10|        [mitogenactivated...|            140|\n",
      "|     [temporal, trend,...|          11|                          []|              0|\n",
      "|                       []|           0|                          []|              0|\n",
      "|     [application, ium...|           6|                          []|              0|\n",
      "|     [constituio, lxic...|           5|        [lexico, desperta...|             90|\n",
      "|     [nanobiocomposite...|           8|        [ternary, nanobio...|             81|\n",
      "|     [feasibility, two...|           9|        [background, aim,...|            127|\n",
      "+-------------------------+------------+----------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf \n",
    "\n",
    "word_count = udf(lambda tokens: len(tokens), IntegerType()) \n",
    "\n",
    "spark_df2 = spark_df2.withColumn('abstract_wcount', word_count(spark_df2['abstract_tokens_filtered_lem']))\n",
    "spark_df2 = spark_df2.withColumn('title_wcount', word_count(spark_df2['title_tokens_filtered_lem']))\n",
    "\n",
    "spark_df2.select('title_tokens_filtered_lem', 'title_wcount', 'abstract_tokens_filtered_lem', 'abstract_wcount').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF for abstract and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maps a sequence of terms to their term frequencies using the hashing trick. \n",
    "hashingTF1 = HashingTF(inputCol=\"abstract_tokens_filtered_lem\", outputCol=\"abstract_tokens_filtered_lem_count\")\n",
    "hashingTF2 = HashingTF(inputCol=\"title_tokens_filtered_lem\", outputCol=\"title_tokens_filtered_lem_count\")\n",
    "featurizedData = hashingTF1.transform(spark_df2)\n",
    "featurizedData = hashingTF2.transform(featurizedData)\n",
    "\n",
    "#Getting IDF\n",
    "idf = IDF(inputCol=\"abstract_tokens_filtered_lem_count\", outputCol=\"abstract_tfidf\")\n",
    "idf2 = IDF(inputCol=\"title_tokens_filtered_lem_count\", outputCol=\"title_tfidf\")\n",
    "\n",
    "idfModel = idf.fit(featurizedData)\n",
    "spark_df3 = idfModel.transform(featurizedData)\n",
    "idfModel = idf2.fit(spark_df3)\n",
    "spark_df3 = idfModel.transform(spark_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------+----------------------------+--------------------+\n",
      "|title_tokens_filtered_lem|         title_tfidf|abstract_tokens_filtered_lem|      abstract_tfidf|\n",
      "+-------------------------+--------------------+----------------------------+--------------------+\n",
      "|     [cause, analysis,...|(262144,[14898,10...|        [problem, budgeta...|(262144,[14898,31...|\n",
      "|     [clair, comme, lu...|(262144,[19604,34...|                          []|      (262144,[],[])|\n",
      "|     [unexpected, deat...|(262144,[5595,660...|        [aim, objective, ...|(262144,[3824,559...|\n",
      "|     [rnaseq, analysis...|(262144,[15207,63...|        [mitogenactivated...|(262144,[3888,685...|\n",
      "|     [temporal, trend,...|(262144,[20908,32...|                          []|      (262144,[],[])|\n",
      "+-------------------------+--------------------+----------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df3.select('title_tokens_filtered_lem',\"title_tfidf\",'abstract_tokens_filtered_lem', \"abstract_tfidf\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Loading model: 2 failed attempts- gensim & pyspark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.getVectors().count() = 14173\n",
      "Time taken to train the word2Vec model: 2.430572183333333 mins\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Learn a mapping from words to Vectors\n",
    "timestart = datetime.datetime.now()\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(windowSize = 5, vectorSize=50, minCount=500, \\\n",
    "                    numPartitions=10, inputCol=\"title_tokens_filtered_lem\", outputCol=\"title_wordVectors\")\n",
    "w2VM = word2Vec.fit(spark_df3)\n",
    "nlpdf = w2VM.transform(spark_df3)\n",
    "\n",
    "# PRINT HOW MUCH TIME IT TOOK TO RUN THE CELL\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = (timeend - timestart).total_seconds() / 60\n",
    "print(\"model.getVectors().count() = {}\".format(w2VM.getVectors().count()))\n",
    "print(\"Time taken to train the word2Vec model: \" + str(timedelta) + \" mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2VM.save('word2vec_model_v50_min500_ws5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_tokens_filtered_lem</th>\n",
       "      <th>title_wordVectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cause, analysis, control, measure, budgetary,...</td>\n",
       "      <td>[-0.5131504601902431, -0.4942763480875227, 1.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[clair, comme, lune, texte, sandra, feder, ill...</td>\n",
       "      <td>[-0.511157146521977, 0.4205438366958073, 0.114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[unexpected, death, medical, ward, night, shif...</td>\n",
       "      <td>[-0.06628441959619523, -0.6808652728796005, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rnaseq, analysis, reveals, mapkkk, family, me...</td>\n",
       "      <td>[-1.1154624477028847, 0.04023271352052689, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[temporal, trend, factor, predicting, use, thr...</td>\n",
       "      <td>[0.1117502748966217, -0.695119781927629, 1.879...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title_tokens_filtered_lem  \\\n",
       "0  [cause, analysis, control, measure, budgetary,...   \n",
       "1  [clair, comme, lune, texte, sandra, feder, ill...   \n",
       "2  [unexpected, death, medical, ward, night, shif...   \n",
       "3  [rnaseq, analysis, reveals, mapkkk, family, me...   \n",
       "4  [temporal, trend, factor, predicting, use, thr...   \n",
       "\n",
       "                                   title_wordVectors  \n",
       "0  [-0.5131504601902431, -0.4942763480875227, 1.4...  \n",
       "1  [-0.511157146521977, 0.4205438366958073, 0.114...  \n",
       "2  [-0.06628441959619523, -0.6808652728796005, 0....  \n",
       "3  [-1.1154624477028847, 0.04023271352052689, -0....  \n",
       "4  [0.1117502748966217, -0.695119781927629, 1.879...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pdf = nlpdf.select('title_tokens_filtered_lem','title_wordVectors').limit(5).toPandas()\n",
    "\n",
    "w2v_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words = \n",
      " ['cause', 'analysis', 'control', 'measure', 'budgetary', 'overrun', 'contemporary', 'estate', 'project']\n",
      "\n",
      "wordvector = \n",
      " [-0.5131504601902431,-0.4942763480875227,1.4551570680406358,1.1178659266895716,0.2359011305703057,-1.0497278140650854,0.6716381013393402,-1.75537637869517,0.046507080396016434,-0.5046158267392052,0.762064516544342,-0.019456326961517334,-0.11661965317196316,0.5606789721382989,0.2459344532754686,0.4041700892978244,-1.2590456853310266,0.7985428124666214,-1.6017301910453372,-0.027074545621871948,1.0152494990163379,0.8104184998406304,-0.5046284794807434,0.43439510133531356,-0.9483143902487224,0.2568034993277656,-0.10435153709517585,0.300687829653422,0.32654528650972575,-0.14927792217996386,1.4431013531155055,-0.26107835272947943,0.36547625395986766,0.6194568806224399,0.855299390024609,-0.4651802248424954,-1.5475021799405415,0.04354015986124674,-0.3665302486883269,-0.48445167144139606,0.0061527084973123335,0.6260736766788694,0.4160140189859602,-0.02192432681719462,1.716097440984514,-0.9715822405285305,0.7327766219774882,-0.8039933774206373,0.3440427780151367,0.8597946812709172]\n"
     ]
    }
   ],
   "source": [
    "print('words = \\n', w2v_pdf.iloc[0,0])\n",
    "print('\\nwordvector = \\n', w2v_pdf.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_terms = w2VM.getVectors().filter((col('word') == 'incident') | (col('word') == 'event')).collect()\n",
    "#w2VM.getVectors().first().vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a,b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "cos_sim_udf = udf(lambda x, y: cos_sim(x,y), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148302086304097"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(test_terms[0].vector, test_terms[1].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('event', 0.8148302435874939),\n",
       " ('baseline', 0.7913722991943359),\n",
       " ('sickness', 0.7755866050720215),\n",
       " ('morbidity', 0.7463832497596741),\n",
       " ('waiting', 0.7460590600967407)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2VM.findSynonymsArray('incident', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coll = cos_sim_udf(array([lit(v) for v in test_terms[0].vector]), \\\n",
    "            array([lit(v) for v in test_terms[1].vector]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'<lambda>(array(-1.4135029315948486, -0.008624415844678879, 1.2140793800354004, -1.8567863702774048, 2.437103748321533, -0.11892496049404144, -2.7120814323425293, 0.4474911093711853, 0.8807857632637024, 1.3831881284713745, 1.9629560708999634, 0.1760571151971817, 0.6771154999732971, 0.9552804827690125, -0.5254846811294556, 1.2863832712173462, 1.2638404369354248, 1.119626522064209, 1.2641879320144653, -0.6995944976806641, 0.4184196889400482, -0.786975085735321, 2.2565484046936035, -0.11487414687871933, ... 26 more fields), array(-1.0543463230133057, -0.5626320242881775, 1.6828505992889404, -1.7820003032684326, 2.4455902576446533, 0.06304009258747101, -1.482363224029541, 1.1752467155456543, 1.9883323907852173, 0.9629970192909241, 1.353745937347412, 0.21402402222156525, 2.288827419281006, 0.9350121021270752, -0.6093250513076782, 0.375243604183197, 0.3302777409553528, 0.7147708535194397, 0.5790965557098389, -0.5335889458656311, 1.4040014743804932, 0.0982133150100708, 1.5089995861053467, 0.8208590745925903, ... 26 more fields))[array][0]'>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_array = test_terms[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+----------+\n",
      "|           word|              vector|     coSim|\n",
      "+---------------+--------------------+----------+\n",
      "|       incident|[-1.4135029315948...|       1.0|\n",
      "|          event|[-1.0543463230133...| 0.8148302|\n",
      "|       baseline|[-0.4684939384460...| 0.7913723|\n",
      "|       sickness|[-0.6365463137626...|0.77558655|\n",
      "|      morbidity|[-0.3282577991485...| 0.7463833|\n",
      "|        waiting|[-1.5830460786819...|0.74605906|\n",
      "|       accident|[-0.5611081719398...|0.73592585|\n",
      "|     cumulative|[-0.0725718364119...|0.73549616|\n",
      "|          crash|[-0.5570912957191...| 0.7273719|\n",
      "|         burden|[-1.8719997406005...| 0.7246494|\n",
      "|hospitalization|[-1.8758287429809...|0.71920043|\n",
      "|         stroke|[-3.7460350990295...| 0.7145015|\n",
      "|      mortality|[-1.8260400295257...| 0.7124914|\n",
      "|         extent|[-0.4867479801177...|0.71128744|\n",
      "|      complaint|[-1.4734418392181...|0.70825344|\n",
      "|      admission|[-2.3375346660614...| 0.7004896|\n",
      "|     vulnerable|[-0.8727873563766...|0.69644785|\n",
      "|        elderly|[-1.5048546791076...|0.69305176|\n",
      "|   occupational|[-1.0991233587265...|  0.690515|\n",
      "|  resuscitation|[-3.0366845130920...| 0.6904518|\n",
      "+---------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2VM.getVectors().withColumn(\"coSim\", cos_sim_udf(col(\"vector\"), array([lit(v) for v in static_array])))\\\n",
    ".orderBy('coSim',ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------+-----+\n",
      "|title_tokens_filtered_lem|   title_wordVectors|coSim|\n",
      "+-------------------------+--------------------+-----+\n",
      "|      [curragh, incident]|[-0.7067514657974...|  1.0|\n",
      "|     [cowm, quarry, in...|[-0.4711676438649...|  1.0|\n",
      "|     [salisbury, incid...|[-0.7067514657974...|  1.0|\n",
      "|     [thalidomide, inc...|[-0.7067514657974...|  1.0|\n",
      "|     [incident, politi...|[-0.7067514657974...|  1.0|\n",
      "|     [tasmanian, incid...|[-0.7067514657974...|  1.0|\n",
      "|     [intracardiac, dr...|[-0.2355838219324...|  1.0|\n",
      "|      [incident, aughrim]|[-0.7067514657974...|  1.0|\n",
      "|        [incident, vichy]|[-0.7067514657974...|  1.0|\n",
      "|     [comrade, liu, di...|[-0.2827005863189...|  1.0|\n",
      "|     [jazz, perfume, i...|[-0.4711676438649...|  1.0|\n",
      "|     [secteur, bocca, ...|[-0.3533757328987...|  1.0|\n",
      "|     [hanford, reacto,...|[-0.4711676438649...|  1.0|\n",
      "|     [taoka, otsu, inc...|[-0.3533757328987...|  1.0|\n",
      "|     [contains, vulgar...|[-0.4711676438649...|  1.0|\n",
      "|      [seekonk, incident]|[-0.7067514657974...|  1.0|\n",
      "|     [pork, chop, inci...|[-0.4711676438649...|  1.0|\n",
      "|          [goa, incident]|[-0.7067514657974...|  1.0|\n",
      "|       [mosley, incident]|[-0.7067514657974...|  1.0|\n",
      "|     [jazz, perfume, i...|[-0.4711676438649...|  1.0|\n",
      "+-------------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlpdf.select('title_tokens_filtered_lem','title_wordVectors')\\\n",
    "    .withColumn(\"coSim\", cos_sim_udf(col(\"title_wordVectors\"), array([lit(v) for v in static_array])))\\\n",
    "    .dropna()\\\n",
    "    .orderBy('coSim',ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create embedding index from file in .txt format. First line contains \n",
    "# # dictionary size and embedding dim. Fields are space separated\n",
    "# import numpy as np \n",
    "# def get_embeddings(file_name):\n",
    "#     embeddings_index = {}\n",
    "#     with open(file_name, encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.rstrip().split(' ')\n",
    "#             if len(values) > 2:\n",
    "#                 embeddings_index[values[0]] = np.asarray(values[1:], dtype=\"float32\")\n",
    "#     return embeddings_index\n",
    "\n",
    "# embeddings_path = 'file:///'\n",
    "# embeddings_index = get_embeddings('/project/msca/kadochnikov/wordvec/glove.6B.300d.txt')\n",
    "\n",
    "# embeddings_dim = len(next(iter(embeddings_index.values())))\n",
    "# embeddings_dim\n",
    "\n",
    "# embeddings_index.get('scholar')\n",
    "\n",
    "# def title2vec(title, vocabulary = embeddings_index.keys(), proxy ='unknown'):\n",
    "#     try:\n",
    "#         vecs = np.array([embeddings_index.get(str(i)) if str(i) in vocabulary else proxy for i in title])\n",
    "#         mean_vec = np.mean(vecs, axis = 0).tolist()\n",
    "#     except:\n",
    "#         mean_vec = [0]*300\n",
    "#     return mean_vec\n",
    "\n",
    "# title2vec_udf = udf(lambda title: title2vec(title, ), ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Word2Vec\n",
    "# loadedWord2Vec = Word2Vec.load('file:///' +'project/msca/kadochnikov/wordvec/glove.6B.300d.txt')\n",
    "# #Word2Vec.load('file:///' +'project/msca/kadochnikov/wordvec/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "# model_google = KeyedVectors.load_word2vec_format('file:///'+'/project/msca/kadochnikov/wordvec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# print(\"Load time {} seconds\".format(time.time() - start_time))\n",
    "\n",
    "# import numpy as np\n",
    "# def title2vec(title):\n",
    "#     try:\n",
    "#         vecs = np.array([model_google[str(i)] for i in title])\n",
    "#         mean_vec = np.mean(vecs, axis = 0).tolist()\n",
    "#     except:\n",
    "#         mean_vec = [0]*300\n",
    "#     return mean_vec\n",
    "\n",
    "# title2vec_udf = udf(lambda title: title2vec(title), ArrayType(FloatType()))\n",
    "\n",
    "# spark_df2.limit(10).withColumn('vecs', title2vec_udf(col('title_tokens_filtered_lem'))).show()\n",
    "# res = title2vec(spark_df2.select('title_tokens_filtered_lem').take(1)[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping SJR to journal name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_map = spark.read.csv('big_data_project/scimagojr_2017.csv', sep = ';',inferSchema=True, header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rank: integer (nullable = true)\n",
      " |-- Sourceid: long (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Issn: string (nullable = true)\n",
      " |-- SJR: string (nullable = true)\n",
      " |-- SJR Best Quartile: string (nullable = true)\n",
      " |-- H index: string (nullable = true)\n",
      " |-- Total Docs. (2017): integer (nullable = true)\n",
      " |-- Total Docs. (3years): integer (nullable = true)\n",
      " |-- Total Refs.: integer (nullable = true)\n",
      " |-- Total Cites (3years): integer (nullable = true)\n",
      " |-- Citable Docs. (3years): integer (nullable = true)\n",
      " |-- Cites / Doc. (2years): string (nullable = true)\n",
      " |-- Ref. / Doc.: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Coverage: string (nullable = true)\n",
      " |-- Categories: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "journal_map.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming column so they dont have same name \n",
    "journal_map2 = journal_map.withColumnRenamed('Title', 'journal_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50554"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlpdf.select('journalName').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs = nlpdf.join(journal_map2, on = spark_df.journalName == journal_map2.journal_name, how = 'left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------+----+\n",
      "|journalName|   venue|journal_name| SJR|\n",
      "+-----------+--------+------------+----+\n",
      "|           |        |        null|null|\n",
      "|           |        |        null|null|\n",
      "|           |        |        null|null|\n",
      "|           |PloS one|        null|null|\n",
      "|           |        |        null|null|\n",
      "+-----------+--------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_dfs.select('journalName', 'venue', 'journal_name', 'SJR').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find 25 percentile to fill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SJR = journal_map.toPandas()[['SJR']]\n",
    "SJR_fillna_val = int(SJR.apply(lambda x: x.str.replace(',','')).dropna().astype(int).quantile(0.25)[0])\n",
    "SJR_fillna_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs = joined_dfs.drop('SJR Best Quartile', 'H index', 'Total Docs. (2017)', 'Total Docs. (3years)', \n",
    "                'Total Refs.', 'Total Cites (3years)', 'Citable Docs. (3years)', 'Cites / Doc. (2years)', \n",
    "                'Ref. / Doc.', 'Country', 'Publisher','Coverage', 'Categories', 'journal_name', 'Type', 'Issn',\n",
    "                            'Rank', 'Sourceid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs = joined_dfs.na.fill({'SJR': SJR_fillna_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs = joined_dfs.withColumn(\"SJR\", joined_dfs[\"SJR\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing some dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('authors', 'array<struct<ids:array<string>,name:string>>'),\n",
       " ('doi', 'string'),\n",
       " ('doiUrl', 'string'),\n",
       " ('entities', 'array<string>'),\n",
       " ('fieldsOfStudy', 'array<string>'),\n",
       " ('id', 'string'),\n",
       " ('inCitations', 'array<string>'),\n",
       " ('journalName', 'string'),\n",
       " ('journalPages', 'string'),\n",
       " ('journalVolume', 'string'),\n",
       " ('outCitations', 'array<string>'),\n",
       " ('paperAbstract', 'string'),\n",
       " ('pdfUrls', 'array<string>'),\n",
       " ('pmid', 'string'),\n",
       " ('s2PdfUrl', 'string'),\n",
       " ('s2Url', 'string'),\n",
       " ('sources', 'array<string>'),\n",
       " ('title', 'string'),\n",
       " ('venue', 'string'),\n",
       " ('year', 'bigint'),\n",
       " ('inCitations_count', 'int'),\n",
       " ('outCitations_count', 'int'),\n",
       " ('abstract_tokens', 'array<string>'),\n",
       " ('title_tokens', 'array<string>'),\n",
       " ('abstract_tokens_filtered', 'array<string>'),\n",
       " ('title_tokens_filtered', 'array<string>'),\n",
       " ('abstract_tokens_filtered_lem', 'array<string>'),\n",
       " ('title_tokens_filtered_lem', 'array<string>'),\n",
       " ('abstract_wcount', 'int'),\n",
       " ('title_wcount', 'int'),\n",
       " ('abstract_tokens_filtered_lem_count', 'vector'),\n",
       " ('title_tokens_filtered_lem_count', 'vector'),\n",
       " ('abstract_tfidf', 'vector'),\n",
       " ('title_tfidf', 'vector'),\n",
       " ('title_wordVectors', 'vector'),\n",
       " ('SJR', 'int')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_dfs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting authors column into authorname & authorID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "joined_dfs = joined_dfs.withColumn('author_ids', F.col('authors.ids'))\n",
    "joined_dfs = joined_dfs.withColumn('author_names', F.col('authors.name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|        author_names|author_count|\n",
      "+--------------------+------------+\n",
      "|       [Yang Li-pin]|           1|\n",
      "|[Sandra V. . Aute...|           4|\n",
      "|[Mns Rn Community...|           2|\n",
      "|[Ya Jun Liu, Miao...|           8|\n",
      "|[Awsan Noman, Moh...|           5|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_dfs = joined_dfs.withColumn('author_count', word_count(joined_dfs['author_names']))\n",
    "\n",
    "joined_dfs.select('author_names' ,'author_count').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONE HOT ENCODING CATEGORICALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|      fieldsOfStudy|  sources|\n",
      "+-------------------+---------+\n",
      "|         [Business]|       []|\n",
      "|          [History]|       []|\n",
      "|         [Medicine]|       []|\n",
      "|[Medicine, Biology]|[Medline]|\n",
      "|         [Medicine]|       []|\n",
      "+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_dfs.select('fieldsOfStudy', 'sources').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|        sources|\n",
      "+---------------+\n",
      "|      [Medline]|\n",
      "|         [DBLP]|\n",
      "|             []|\n",
      "|[Medline, DBLP]|\n",
      "|[DBLP, Medline]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_dfs.select('sources').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's unpack the arrays and turn them into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to unpack list intro string joined with ', ' \n",
    "udf_unpack = udf(lambda listt: ', '.join(listt), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs = joined_dfs.withColumn('fieldsOfStudy', udf_unpack(col('fieldsOfStudy')))\n",
    "joined_dfs = joined_dfs.withColumn('sources', udf_unpack(col('sources')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs = joined_dfs.withColumn('sources', regexp_replace(col(\"sources\"), r\"^\\s*$\", \"no_source\"))\n",
    "joined_dfs = joined_dfs.withColumn('fieldsOfStudy', regexp_replace(col(\"fieldsOfStudy\"), r\"^\\s*$\", \"no_FoS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      sources|\n",
      "+-------------+\n",
      "|Medline, DBLP|\n",
      "|      Medline|\n",
      "|         DBLP|\n",
      "|DBLP, Medline|\n",
      "|    no_source|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_dfs.select('sources').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+-----------------+--------------------+--------------------+-----------+------------+-------------+--------------------+--------------------+--------------------+--------+--------+--------------------+---------+--------------------+--------+----+-----------------+------------------+--------------------+--------------------+------------------------+---------------------+----------------------------+-------------------------+---------------+------------+----------------------------------+-------------------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+------------+----------------+----------+-----------------+-------------+\n",
      "|             authors|                 doi|              doiUrl|entities|    fieldsOfStudy|                  id|         inCitations|journalName|journalPages|journalVolume|        outCitations|       paperAbstract|             pdfUrls|    pmid|s2PdfUrl|               s2Url|  sources|               title|   venue|year|inCitations_count|outCitations_count|     abstract_tokens|        title_tokens|abstract_tokens_filtered|title_tokens_filtered|abstract_tokens_filtered_lem|title_tokens_filtered_lem|abstract_wcount|title_wcount|abstract_tokens_filtered_lem_count|title_tokens_filtered_lem_count|      abstract_tfidf|         title_tfidf|   title_wordVectors|SJR|          author_ids|        author_names|author_count|fieldsOfStudyIdx|sourcesIdx| fieldsOfStudyVec|   sourcesVec|\n",
      "+--------------------+--------------------+--------------------+--------+-----------------+--------------------+--------------------+-----------+------------+-------------+--------------------+--------------------+--------------------+--------+--------+--------------------+---------+--------------------+--------+----+-----------------+------------------+--------------------+--------------------+------------------------+---------------------+----------------------------+-------------------------+---------------+------------+----------------------------------+-------------------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+------------+----------------+----------+-----------------+-------------+\n",
      "|[[[52602509], Yan...|                    |                    |      []|         Business|b9c27ac1bb8b3ec9d...|                  []|           |            |             |                  []|the problem of bu...|                  []|        |        |https://semantics...|no_source|cause analysis an...|        |2014|                0|                 0|[the, problem, of...|[cause, analysis,...|    [problem, budgeta...| [cause, analysis,...|        [problem, budgeta...|     [cause, analysis,...|             61|           9|              (262144,[14898,31...|           (262144,[14898,10...|(262144,[14898,31...|(262144,[14898,10...|[-0.5131504601902...|124|        [[52602509]]|       [Yang Li-pin]|           1|            17.0|       0.0|(2179,[17],[1.0])|(4,[0],[1.0])|\n",
      "|[[[122800702], Sa...|                    |                    |      []|          History|91cac7b12800a6238...|                  []|           |            |             |                  []|                    |                  []|        |        |https://semantics...|no_source|clair comme lune ...|        |2016|                0|                 0|                  []|[clair, comme, lu...|                      []| [clair, comme, lu...|                          []|     [clair, comme, lu...|              0|          14|                    (262144,[],[])|           (262144,[19604,34...|      (262144,[],[])|(262144,[19604,34...|[-0.5111571465219...|124|[[122800702], [12...|[Sandra V. . Aute...|           4|             9.0|       0.0| (2179,[9],[1.0])|(4,[0],[1.0])|\n",
      "|[[[1392355377], M...|  10.1111/jocn.12477|https://doi.org/1...|      []|         Medicine|412b10a92babf3509...|                  []|           |            |             |                  []|aims and objectiv...|                  []|        |        |https://semantics...|no_source|unexpected deaths...|        |2013|                0|                 0|[aims, and, objec...|[unexpected, deat...|    [aims, objectives...| [unexpected, deat...|        [aim, objective, ...|     [unexpected, deat...|            197|          10|              (262144,[3824,559...|           (262144,[5595,660...|(262144,[3824,559...|(262144,[5595,660...|[-0.0662844195961...|124|[[1392355377], [1...|[Mns Rn Community...|           2|             0.0|       0.0| (2179,[0],[1.0])|(4,[0],[1.0])|\n",
      "|[[[47909706], Ya ...|10.1371/journal.p...|https://doi.org/1...|      []|Medicine, Biology|0de1cecd2ed49812f...|[8ecaab2a03953fa5...|           |    551 - 60|           10|[cf5cdba6424524ee...|the mitogenactiva...|[http://ftp.ncbi....|26599013|        |https://semantics...|  Medline|rnaseq analysis r...|PloS one|2015|               13|                69|[the, mitogenacti...|[rnaseq, analysis...|    [mitogenactivated...| [rnaseq, analysis...|        [mitogenactivated...|     [rnaseq, analysis...|            140|          10|              (262144,[3888,685...|           (262144,[15207,63...|(262144,[3888,685...|(262144,[15207,63...|[-1.1154624477028...|124|[[47909706], [215...|[Ya Jun Liu, Miao...|           8|            16.0|       1.0|(2179,[16],[1.0])|(4,[1],[1.0])|\n",
      "|[[[47264369], Aws...|10.1016/S0735-109...|https://doi.org/1...|      []|         Medicine|1d9fee40f59bf00eb...|                  []|           |            |             |                  []|                    |                  []|        |        |https://semantics...|no_source|temporal trends a...|        |2012|                0|                 0|                  []|[temporal, trends...|                      []| [temporal, trends...|                          []|     [temporal, trend,...|              0|          11|                    (262144,[],[])|           (262144,[20908,32...|      (262144,[],[])|(262144,[20908,32...|[0.11175027489662...|124|[[47264369], [654...|[Awsan Noman, Moh...|           5|             0.0|       0.0| (2179,[0],[1.0])|(4,[0],[1.0])|\n",
      "+--------------------+--------------------+--------------------+--------+-----------------+--------------------+--------------------+-----------+------------+-------------+--------------------+--------------------+--------------------+--------+--------+--------------------+---------+--------------------+--------+----+-----------------+------------------+--------------------+--------------------+------------------------+---------------------+----------------------------+-------------------------+---------------+------------+----------------------------------+-------------------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+------------+----------------+----------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "#convert relevant categorical into one hot encoded\n",
    "indexer1 = StringIndexer(inputCol=\"fieldsOfStudy\", outputCol=\"fieldsOfStudyIdx\").setHandleInvalid(\"skip\")\n",
    "indexer2 = StringIndexer(inputCol=\"sources\", outputCol=\"sourcesIdx\").setHandleInvalid(\"skip\")\n",
    "\n",
    "#gather all indexers as inputs to the One Hot Encoder\n",
    "inputs = [indexer1.getOutputCol(), indexer2.getOutputCol()]\n",
    "\n",
    "#create the one hot encoder\n",
    "encoder = OneHotEncoderEstimator(inputCols=inputs,  \\\n",
    "                                 outputCols=[\"fieldsOfStudyVec\", \"sourcesVec\"])\n",
    "\n",
    "#run it through a pipeline\n",
    "pipeline = Pipeline(stages=[indexer1, indexer2, encoder])\n",
    "encodedData = pipeline.fit(joined_dfs).transform(joined_dfs)\n",
    "\n",
    "#we have removed NAs so dont need to impute missing values.\n",
    "#pipeline = pipeline.na.fill(0) \n",
    "\n",
    "encodedData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECTING COLUMNS FOR MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data = encodedData.select(\n",
    "    'id', 'year',\n",
    " 'inCitations_count',\n",
    " 'outCitations_count','abstract_wcount',\n",
    " 'title_wcount','abstract_tfidf',\n",
    " 'title_tfidf',\n",
    " 'SJR','author_count','fieldsOfStudyVec',\n",
    " 'sourcesVec', 'title_wordVectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>inCitations_count</th>\n",
       "      <th>outCitations_count</th>\n",
       "      <th>abstract_wcount</th>\n",
       "      <th>title_wcount</th>\n",
       "      <th>abstract_tfidf</th>\n",
       "      <th>title_tfidf</th>\n",
       "      <th>SJR</th>\n",
       "      <th>author_count</th>\n",
       "      <th>fieldsOfStudyVec</th>\n",
       "      <th>sourcesVec</th>\n",
       "      <th>title_wordVectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b9c27ac1bb8b3ec9d4faa06ed7163d002c8967bb</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>[-0.38373208294312156, 0.34186492363611853, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91cac7b12800a6238d6bb3bcdf338f8a1556b912</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>[-1.823535817010062, -0.9715813568660191, -1.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>412b10a92babf350975dd98e58039891d792df6d</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>10</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>124</td>\n",
       "      <td>2</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>[-0.9904607474803925, 0.5073053002357483, 1.85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0de1cecd2ed49812f3a55e4b78edf9ffd292618c</td>\n",
       "      <td>2015</td>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "      <td>140</td>\n",
       "      <td>10</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>124</td>\n",
       "      <td>8</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>[1.2772655487060547, 2.0660125970840455, 1.053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1d9fee40f59bf00eb85ac8dd0e75d0514648c7a7</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>124</td>\n",
       "      <td>5</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>[-0.1645142219283364, 1.3447277735580099, 0.51...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  year  inCitations_count  \\\n",
       "0  b9c27ac1bb8b3ec9d4faa06ed7163d002c8967bb  2014                  0   \n",
       "1  91cac7b12800a6238d6bb3bcdf338f8a1556b912  2016                  0   \n",
       "2  412b10a92babf350975dd98e58039891d792df6d  2013                  0   \n",
       "3  0de1cecd2ed49812f3a55e4b78edf9ffd292618c  2015                 13   \n",
       "4  1d9fee40f59bf00eb85ac8dd0e75d0514648c7a7  2012                  0   \n",
       "\n",
       "   outCitations_count  abstract_wcount  title_wcount  \\\n",
       "0                   0               61             9   \n",
       "1                   0                0            14   \n",
       "2                   0              197            10   \n",
       "3                  69              140            10   \n",
       "4                   0                0            11   \n",
       "\n",
       "                                      abstract_tfidf  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                         title_tfidf  SJR  author_count  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  124             1   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  124             4   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  124             2   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  124             8   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  124             5   \n",
       "\n",
       "                                    fieldsOfStudyVec            sourcesVec  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (1.0, 0.0, 0.0, 0.0)   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (1.0, 0.0, 0.0, 0.0)   \n",
       "2  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (1.0, 0.0, 0.0, 0.0)   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (0.0, 1.0, 0.0, 0.0)   \n",
       "4  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  (1.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                                   title_wordVectors  \n",
       "0  [-0.38373208294312156, 0.34186492363611853, 0....  \n",
       "1  [-1.823535817010062, -0.9715813568660191, -1.2...  \n",
       "2  [-0.9904607474803925, 0.5073053002357483, 1.85...  \n",
       "3  [1.2772655487060547, 2.0660125970840455, 1.053...  \n",
       "4  [-0.1645142219283364, 1.3447277735580099, 0.51...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_final = modeling_data.limit(5).toPandas()\n",
    "pdf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "#gather feature vector and identify features\n",
    "assembler = VectorAssembler(inputCols = ['year', 'outCitations_count','abstract_wcount','title_wcount','abstract_tfidf',\\\n",
    "                                         'title_tfidf','SJR','author_count','fieldsOfStudyVec', 'sourcesVec', \\\n",
    "                                         'title_wordVectors'], \\\n",
    "                            outputCol = 'features')\n",
    "\n",
    "\n",
    "modeling_data = assembler.transform(modeling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'year',\n",
       " 'inCitations_count',\n",
       " 'outCitations_count',\n",
       " 'abstract_wcount',\n",
       " 'title_wcount',\n",
       " 'abstract_tfidf',\n",
       " 'title_tfidf',\n",
       " 'SJR',\n",
       " 'author_count',\n",
       " 'fieldsOfStudyVec',\n",
       " 'sourcesVec',\n",
       " 'title_wordVectors',\n",
       " 'features']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeling_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o978.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 4 times, most recent failure: Lost task 0.3 in stage 81.0 (TID 4198, hd03.rcc.local, executor 486): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<year_double_VectorAssembler_204f1b780323:double,outCitations_count_double_VectorAssembler_204f1b780323:double,abstract_wcount_double_VectorAssembler_204f1b780323:double,title_wcount_double_VectorAssembler_204f1b780323:double,abstract_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SJR_double_VectorAssembler_204f1b780323:double,author_count_double_VectorAssembler_204f1b780323:double,fieldsOfStudyVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,sourcesVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_wordVectors:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<year_double_VectorAssembler_204f1b780323:double,outCitations_count_double_VectorAssembler_204f1b780323:double,abstract_wcount_double_VectorAssembler_204f1b780323:double,title_wcount_double_VectorAssembler_204f1b780323:double,abstract_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SJR_double_VectorAssembler_204f1b780323:double,author_count_double_VectorAssembler_204f1b780323:double,fieldsOfStudyVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,sourcesVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_wordVectors:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-541d18f10b2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#split data into train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodeling_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o978.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 4 times, most recent failure: Lost task 0.3 in stage 81.0 (TID 4198, hd03.rcc.local, executor 486): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<year_double_VectorAssembler_204f1b780323:double,outCitations_count_double_VectorAssembler_204f1b780323:double,abstract_wcount_double_VectorAssembler_204f1b780323:double,title_wcount_double_VectorAssembler_204f1b780323:double,abstract_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SJR_double_VectorAssembler_204f1b780323:double,author_count_double_VectorAssembler_204f1b780323:double,fieldsOfStudyVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,sourcesVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_wordVectors:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<year_double_VectorAssembler_204f1b780323:double,outCitations_count_double_VectorAssembler_204f1b780323:double,abstract_wcount_double_VectorAssembler_204f1b780323:double,title_wcount_double_VectorAssembler_204f1b780323:double,abstract_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SJR_double_VectorAssembler_204f1b780323:double,author_count_double_VectorAssembler_204f1b780323:double,fieldsOfStudyVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,sourcesVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_wordVectors:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "#split data into train and test\n",
    "train_df, test_df = modeling_data.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Length = ', train_df.count())\n",
    "print('Test Length = ', test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#Elastic Net\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='inCitations_count', regParam=0.3, maxIter=10)\n",
    "lrm = lr.fit(train_df)\n",
    "\n",
    "#coefficients\n",
    "print(\"Coefficients: \" + str(lrm.coefficients))\n",
    "print(\"Intercept: \" + str(lrm.intercept))\n",
    "\n",
    "#model summary\n",
    "print(\"RMSE: %f\" % lrm.summary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % lrm.summary.r2)\n",
    "\n",
    "#p-values are not provided in this model for the solver being used\n",
    "#print(\"pValues: \" + str(lrm.summary.pValues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = \n",
    "# ['authors','doi','doiUrl','entities','journalPages',\n",
    "#  'journalVolume','paperAbstract',\n",
    "#  'pdfUrls',\n",
    "#  'pmid',\n",
    "#  's2PdfUrl',\n",
    "#  's2Url','title','abstract_tokens',\n",
    "#  'title_tokens',\n",
    "#  'abstract_tokens_filtered',\n",
    "#  'title_tokens_filtered',\n",
    "#  'abstract_tokens_filtered_lem',\n",
    "#  'title_tokens_filtered_lem',\n",
    "#  'abstract_tokens_filtered_lem_count',\n",
    "#  'title_tokens_filtered_lem_count',\n",
    "#  'author_ids',\n",
    "#  'author_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1026.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 86 in stage 83.0 failed 4 times, most recent failure: Lost task 86.3 in stage 83.0 (TID 4408, hd06.rcc.local, executor 790): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<year_double_VectorAssembler_204f1b780323:double,outCitations_count_double_VectorAssembler_204f1b780323:double,abstract_wcount_double_VectorAssembler_204f1b780323:double,title_wcount_double_VectorAssembler_204f1b780323:double,abstract_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SJR_double_VectorAssembler_204f1b780323:double,author_count_double_VectorAssembler_204f1b780323:double,fieldsOfStudyVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,sourcesVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_wordVectors:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<year_double_VectorAssembler_204f1b780323:double,outCitations_count_double_VectorAssembler_204f1b780323:double,abstract_wcount_double_VectorAssembler_204f1b780323:double,title_wcount_double_VectorAssembler_204f1b780323:double,abstract_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SJR_double_VectorAssembler_204f1b780323:double,author_count_double_VectorAssembler_204f1b780323:double,fieldsOfStudyVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,sourcesVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_wordVectors:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-bdd5640b5381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodeling_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'big_data_project/modeling_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1026.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 86 in stage 83.0 failed 4 times, most recent failure: Lost task 86.3 in stage 83.0 (TID 4408, hd06.rcc.local, executor 790): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<year_double_VectorAssembler_204f1b780323:double,outCitations_count_double_VectorAssembler_204f1b780323:double,abstract_wcount_double_VectorAssembler_204f1b780323:double,title_wcount_double_VectorAssembler_204f1b780323:double,abstract_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SJR_double_VectorAssembler_204f1b780323:double,author_count_double_VectorAssembler_204f1b780323:double,fieldsOfStudyVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,sourcesVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_wordVectors:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (struct<year_double_VectorAssembler_204f1b780323:double,outCitations_count_double_VectorAssembler_204f1b780323:double,abstract_wcount_double_VectorAssembler_204f1b780323:double,title_wcount_double_VectorAssembler_204f1b780323:double,abstract_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,SJR_double_VectorAssembler_204f1b780323:double,author_count_double_VectorAssembler_204f1b780323:double,fieldsOfStudyVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,sourcesVec:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,title_wordVectors:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "modeling_data.write.format('parquet').save('big_data_project/modeling_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "Found 121 items\n",
      "-rw-r--r--   3 nazihkalo nazihkalo          0 2020-02-17 16:22 modeling_data/_SUCCESS\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00000-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.8 M 2020-02-17 16:21 modeling_data/part-00001-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00002-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00003-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00004-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00005-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00006-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00007-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00008-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00009-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00010-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00011-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00012-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00013-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00014-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00015-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00016-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00017-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00018-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00019-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00020-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00021-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00022-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.4 M 2020-02-17 16:21 modeling_data/part-00023-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00024-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00025-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.5 M 2020-02-17 16:21 modeling_data/part-00026-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00027-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00028-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00029-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.8 M 2020-02-17 16:21 modeling_data/part-00030-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00031-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00032-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.8 M 2020-02-17 16:22 modeling_data/part-00033-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00034-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.5 M 2020-02-17 16:21 modeling_data/part-00035-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00036-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00037-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00038-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.8 M 2020-02-17 16:21 modeling_data/part-00039-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.5 M 2020-02-17 16:22 modeling_data/part-00040-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00041-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00042-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00043-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00044-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.8 M 2020-02-17 16:21 modeling_data/part-00045-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00046-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.5 M 2020-02-17 16:22 modeling_data/part-00047-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00048-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00049-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00050-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.4 M 2020-02-17 16:21 modeling_data/part-00051-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00052-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00053-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00054-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00055-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00056-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00057-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.5 M 2020-02-17 16:21 modeling_data/part-00058-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00059-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00060-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00061-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00062-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00063-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00064-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00065-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00066-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00067-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00068-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00069-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00070-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.8 M 2020-02-17 16:21 modeling_data/part-00071-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00072-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00073-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00074-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:21 modeling_data/part-00075-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00076-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.5 M 2020-02-17 16:22 modeling_data/part-00077-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:21 modeling_data/part-00078-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00079-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00080-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00081-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00082-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00083-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00084-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.8 M 2020-02-17 16:22 modeling_data/part-00085-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00086-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00087-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00088-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.5 M 2020-02-17 16:22 modeling_data/part-00089-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00090-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00091-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00092-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00093-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00094-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00095-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00096-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00097-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00098-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00099-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00100-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00101-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00102-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00103-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00104-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.6 M 2020-02-17 16:22 modeling_data/part-00105-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.4 M 2020-02-17 16:22 modeling_data/part-00106-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00107-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00108-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     20.7 M 2020-02-17 16:22 modeling_data/part-00109-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     19.2 M 2020-02-17 16:22 modeling_data/part-00110-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.6 M 2020-02-17 16:22 modeling_data/part-00111-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.5 M 2020-02-17 16:22 modeling_data/part-00112-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.5 M 2020-02-17 16:22 modeling_data/part-00113-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.6 M 2020-02-17 16:22 modeling_data/part-00114-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.5 M 2020-02-17 16:22 modeling_data/part-00115-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.3 M 2020-02-17 16:22 modeling_data/part-00116-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.1 M 2020-02-17 16:22 modeling_data/part-00117-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.0 M 2020-02-17 16:22 modeling_data/part-00118-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n",
      "-rw-r--r--   3 nazihkalo nazihkalo     18.0 M 2020-02-17 16:22 modeling_data/part-00119-98e10948-a5e6-4de3-aacc-9a89a2874ca9-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h modeling_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "|summary|inCitations_count|outCitations_count|  abstract_wcount|     title_wcount|\n",
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "|  count|          9980999|           9980999|          9980999|          9980999|\n",
      "|   mean|3.534729739978934|3.5169331246301097|52.31906465475049|7.195430236993311|\n",
      "| stddev|33.82246627659463|14.105878711775079|70.56536328977172|4.034703276166936|\n",
      "|    min|                0|                 0|                0|                0|\n",
      "|    max|            36091|              2497|             1266|               40|\n",
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encodedData.select('inCitations_count', 'outCitations_count', 'abstract_wcount','title_wcount').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Encoding some categoricals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = encodedData.select('id', 'title', 'year', 'fieldsOfStudy', 'paperAbstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+-----------------+--------------------+\n",
      "|                  id|               title|year|    fieldsOfStudy|       paperAbstract|\n",
      "+--------------------+--------------------+----+-----------------+--------------------+\n",
      "|b9c27ac1bb8b3ec9d...|Cause Analysis an...|2014|         Business|The problem of bu...|\n",
      "|91cac7b12800a6238...|Clair comme lune ...|2016|          History|                    |\n",
      "|412b10a92babf3509...|Unexpected deaths...|2013|         Medicine|Aims and objectiv...|\n",
      "|0de1cecd2ed49812f...|RNA-Seq Analysis ...|2015|Medicine, Biology|The mitogen-activ...|\n",
      "|1d9fee40f59bf00eb...|Temporal Trends a...|2012|         Medicine|                    |\n",
      "+--------------------+--------------------+----+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vertices.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|         inCitations|        outCitations|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|b9c27ac1bb8b3ec9d...|                  []|                  []|\n",
      "|91cac7b12800a6238...|                  []|                  []|\n",
      "|412b10a92babf3509...|                  []|                  []|\n",
      "|0de1cecd2ed49812f...|[8ecaab2a03953fa5...|[cf5cdba6424524ee...|\n",
      "|1d9fee40f59bf00eb...|                  []|                  []|\n",
      "|5e33c40c33d9e4753...|                  []|                  []|\n",
      "|d477d10c48912335d...|                  []|                  []|\n",
      "|b87f8f3daaa92d3bc...|                  []|                  []|\n",
      "|598567aa0229a6b31...|[ecec08f5ca15e07d...|                  []|\n",
      "|b790f30e811445a68...|[2da3df885d1f032d...|                  []|\n",
      "|69a00b492e0ba6e25...|                  []|                  []|\n",
      "|6b75f9994d131b2ee...|[ac89cd3e34dbc788...|                  []|\n",
      "|9eda969296439bf4c...|                  []|[411f288349de96c0...|\n",
      "|47bee8ebe530dc4ff...|[e5951f7aa9eef9c4...|[3dc9587f4b564c90...|\n",
      "|79ac36d7f74cf8d83...|                  []|                  []|\n",
      "|e079322082ef30858...|                  []|                  []|\n",
      "|c5bf1145b594c518b...|                  []|[0742fa40bf9be455...|\n",
      "|bd72c9e0434214845...|                  []|                  []|\n",
      "|d125a6c2adbfa1932...|                  []|                  []|\n",
      "|4acf97f40b5e9b991...|                  []|                  []|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encodedData.select('id', 'inCitations', 'outCitations').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Getting the inCitations id -> (cited) -> article id \n",
    "edges = encodedData.select(explode('inCitations').alias('src'), encodedData.id.alias('dst')).withColumn('type', lit('cited'))\n",
    "\n",
    "#Getting the article id -> (cited) -> outCitations id\n",
    "edges2 = encodedData.select(encodedData.id.alias('src'), explode('outCitations').alias('dst')).withColumn('type', lit('cited'))\n",
    "\n",
    "#Union of these two \n",
    "edges_total = edges.union(edges2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+-----+\n",
      "|src                                     |dst                                     |type |\n",
      "+----------------------------------------+----------------------------------------+-----+\n",
      "|8ecaab2a03953fa5cf08ab5db2bb49c16d90527d|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "|8ea68d48d5595730f082f625a2ad759f934411cf|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "|766a2b4d54b541c737f6a6ec81ad487b56dc83ed|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "|8479962401362c07f2d6cbff871a11cbd7a4913f|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "|a0ab0cf6c8fe0b1e50407576ba5b80f0f3f58476|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "+----------------------------------------+----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_total.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                  id|degree|\n",
      "+--------------------+------+\n",
      "|0527b65da84f42a39...|    28|\n",
      "|a830e94246ea34694...|    12|\n",
      "|61fa386d2187bebaf...|     6|\n",
      "|4bebbb2592590a23c...|     1|\n",
      "|ff6007e947ed35055...|     5|\n",
      "|16d5358ed974cdefc...|     2|\n",
      "|ba562608abe6a7e81...|     1|\n",
      "|19f599b95b54304ec...|    21|\n",
      "|5e5ce011e6f57a24d...|     1|\n",
      "|6ccc8a7f7c1362cb9...|     2|\n",
      "|5c8faef8512d3ac63...|     6|\n",
      "|932a7e026b2acd162...|    20|\n",
      "|ca82c02aef19b1e64...|    40|\n",
      "|7d306abf7ff739313...|     8|\n",
      "|d23cb008042ae46a8...|    63|\n",
      "|dc19d325198236660...|    67|\n",
      "|56f289078d090d0ea...|     2|\n",
      "|2227a64a2d770f06d...|    29|\n",
      "|3c0b62bc2d1d09a0c...|     7|\n",
      "|6e1516a2c25889ffd...|     3|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(vertices, edges_total)\n",
    "## Take a look at the DataFrames\n",
    "# g.vertices.show()\n",
    "# g.edges.show()\n",
    "## Check the number of edges of each vertex\n",
    "g.degrees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### indegree = The number of edges directed into a vertex in a directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|inDegree|\n",
      "+--------------------+--------+\n",
      "|83730969c0686b1d1...|   38079|\n",
      "|abd1c342495432171...|   32420|\n",
      "|13d4c2f76a7c1a4d0...|   20763|\n",
      "|a512385be058b1e2e...|   17634|\n",
      "|a411f6a0e6473137a...|   17370|\n",
      "|87f40e6f3022adbc1...|   12898|\n",
      "|73679f1ba00de9e73...|   11428|\n",
      "|e6dd0e6cf076b1207...|   11276|\n",
      "|d2860a370a0386c57...|   10815|\n",
      "|6a17ebeeb80cd696b...|    9429|\n",
      "|6da2a9ffa23a59523...|    9197|\n",
      "|10d6778bc45aebcd5...|    9120|\n",
      "|490020c0d4fa1eb85...|    9028|\n",
      "|fc448a7db5a2fac24...|    8676|\n",
      "|54205667c1f65a320...|    8611|\n",
      "|cc90910b6e31fe44c...|    8231|\n",
      "|a42d6065d0b1a31c5...|    8182|\n",
      "|6889e5a22598521d8...|    7656|\n",
      "|0e68beebb4c7ccbd9...|    7486|\n",
      "|9f649b234f5ebf207...|    7350|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.filter(\"inDegree >= 10\").sort(\"inDegree\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='83730969c0686b1d185bcca39f9b5743fa53ebc1')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.inDegrees.filter(\"inDegree >= 10\").sort(\"inDegree\", ascending=False).select('id').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------------------------------------------------------+----+----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id                                      |title                                                   |year|fieldsOfStudy   |paperAbstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+----------------------------------------+--------------------------------------------------------+----+----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|83730969c0686b1d185bcca39f9b5743fa53ebc1|R: A language and environment for statistical computing.|2014|Computer Science|Copyright () 19992012 R Foundation for Statistical Computing. Permission is granted to make and distribute verbatim copies of this manual provided the copyright notice and this permission notice are preserved on all copies. Permission is granted to copy and distribute modified versions of this manual under the conditions for verbatim copying, provided that the entire resulting derived work is distributed under the terms of a permission notice identical to this one. Permission is granted to copy and distribute translations of this manual into another language, under the above conditions for modified versions, except that this permission notice may be stated in a translation approved by the R Core Team.|\n",
      "+----------------------------------------+--------------------------------------------------------+----+----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.vertices.filter('id == \"83730969c0686b1d185bcca39f9b5743fa53ebc1\"').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50554"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df3.select('journalName').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import explode\n",
    "# id_authors = spark_df2.select(explode('author_ids'))\n",
    "# spark_df2.select('author_names', explode('author_names')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert the list + string into string\n",
    "# spark_df3 = spark_df3.withColumn(\"entities\", spark_df3[\"entities\"].cast(StringType()))\n",
    "# spark_df3 = spark_df3.withColumn(\"fieldsOfStudy\", spark_df3[\"fieldsOfStudy\"].cast(StringType()))\n",
    "# spark_df3 = spark_df3.withColumn(\"pdfUrls\", spark_df3[\"pdfUrls\"].cast(StringType()))\n",
    "# spark_df3 = spark_df3.withColumn(\"sources\", spark_df3[\"sources\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fields_count = spark_df3.groupby('fieldsOfStudy').count().sort('count', ascending = False)\n",
    "#convert Pyspark dataframe to Pandas dataframe for plotting\n",
    "pdf = fields_count.toPandas()\n",
    "pdf[:20].plot(kind= 'bar', x='fieldsOfStudy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nans by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# spark_df3.select([count(when(isnan(c), c)).alias(c) for c in spark_df3.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISTRIBUTION OF inCitations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Doing the heavy lifting in Spark. We could leverage the `histogram` function from the RDD api\n",
    "\n",
    "inCitations_count_histogram = spark_df3.select('inCitations_count').rdd.flatMap(lambda x: x).histogram(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe8371e5908>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the Computed Histogram into a Pandas Dataframe for plotting\n",
    "pd.DataFrame(\n",
    "    list(zip(*inCitations_count_histogram)), \n",
    "    columns=['bin', 'frequency']\n",
    ").set_index(\n",
    "    'bin'\n",
    ").plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+----------------------+----------------------+------------------------+\n",
      "|min(inCitations_count)|max(inCitations_count)|avg(inCitations_count)|sum(inCitations_count)|count(inCitations_count)|\n",
      "+----------------------+----------------------+----------------------+----------------------+------------------------+\n",
      "|0                     |36091                 |3.5324064204140146    |35242780              |9976989                 |\n",
      "+----------------------+----------------------+----------------------+----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark_df3.agg(F.min(spark_df3.inCitations_count),F.max(spark_df3.inCitations_count), \\\n",
    "              F.avg(spark_df3.inCitations_count),F.sum(spark_df3.inCitations_count), \\\n",
    "             F.count(spark_df3.inCitations_count)).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9976989"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df3.write.csv('corpus0_to_9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df4.write.format('parquet').save('all_papers_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hive_context = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| databaseName|\n",
      "+-------------+\n",
      "|     aamorris|\n",
      "|    ababikova|\n",
      "|          abb|\n",
      "|      abertin|\n",
      "| abhishekchat|\n",
      "|      achenad|\n",
      "|  adeshghadge|\n",
      "|      adetola|\n",
      "| adhamsuliman|\n",
      "|   aditilakra|\n",
      "|       aghose|\n",
      "|       ahphan|\n",
      "|      airline|\n",
      "|       alphan|\n",
      "|       aluong|\n",
      "|alvinharyanto|\n",
      "|       amant3|\n",
      "|    amiparikh|\n",
      "|amoliterno945|\n",
      "|     andmoral|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show databaes\n",
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "# from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# # Clean text\n",
    "# df_clean = spark_df2.withColumn('title_clean', (lower(regexp_replace('title', \"[^a-zA-Z\\\\s]\", \"\")).alias('title')))\n",
    "\n",
    "# # Tokenize text\n",
    "# tokenizer = Tokenizer(inputCol='title_clean', outputCol='title_clean_token')\n",
    "# df_words_token = tokenizer.transform(df_clean)\n",
    "\n",
    "# # Remove stop words\n",
    "# remover = StopWordsRemover(inputCol='title_clean_token', outputCol='title_clean_token_sw')\n",
    "# df_words_no_stopw = remover.transform(df_words_token)\n",
    "\n",
    "# # Stem text\n",
    "# stemmer = SnowballStemmer(language='english')\n",
    "# stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "# df_stemmed = df_words_no_stopw.withColumn(\"title_clean_token_stemmed\", stemmer_udf(\"title_clean_token_sw\"))\n",
    "\n",
    "# # Filter length word > 3\n",
    "# filter_length_udf = udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
    "# df_final_words = df_stemmed.withColumn('title_complete', filter_length_udf(col('title_clean_token_stemmed')))\n",
    "\n",
    "#####################################################################################################################\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "# import string\n",
    "# nltk.download('wordnet')\n",
    "# stop_words=set(stopwords.words('english'))\n",
    "# list_punct=list(string.punctuation)\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def clean_text(x):\n",
    "#     filteredSentence = [w for w in x if not w in stop_words]\n",
    "#     filtered = [''.join(c for c in s if c not in list_punct) for s in filteredSentence] \n",
    "#     filtered_space = [s for s in filtered if s] #remove empty space\n",
    "#     finalLem = [lemmatizer.lemmatize(s) for s in filtered_space]\n",
    "#     return finalLem\n",
    "\n",
    "# clean_text_udf = udf(lambda tokens: clean_text(tokens), ArrayType(StringType())) \n",
    "\n",
    "#clean_text(spark_df2.limit(1).select('title_tokens').collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ANALYSIS OF JOURNAL NAMES\n",
    "# spark_df3.groupby('journalName').agg(F.min(spark_df3.inCitations_count),F.max(spark_df3.inCitations_count), \\\n",
    "#               F.avg(spark_df3.inCitations_count),F.sum(spark_df3.inCitations_count), \\\n",
    "#              F.count(spark_df3.inCitations_count)).sort('count(inCitations_count)', ascending = False).show(truncate = False)\n",
    "\n",
    "# fields_count = spark_df3.groupby('journalName').count().sort('count', ascending = False)\n",
    "# #convert Pyspark dataframe to Pandas dataframe for plotting\n",
    "# pdf = fields_count.toPandas()\n",
    "# pdf[:20].plot(kind= 'bar', x='journalName')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
