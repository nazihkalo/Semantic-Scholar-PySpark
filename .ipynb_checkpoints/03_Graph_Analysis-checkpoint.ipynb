{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Building & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#Graph network imports\n",
    "from graphframes import *\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "from pyspark.sql.functions import udf #user defined function\n",
    "from pyspark.sql.types import * #Import types == IntegerType, StringType etc.\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.json('big_data_project/unzipped_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ids: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- doiUrl: string (nullable = true)\n",
      " |-- entities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fieldsOfStudy: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- journalName: string (nullable = true)\n",
      " |-- journalPages: string (nullable = true)\n",
      " |-- journalVolume: string (nullable = true)\n",
      " |-- outCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- paperAbstract: string (nullable = true)\n",
      " |-- pdfUrls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- s2PdfUrl: string (nullable = true)\n",
      " |-- s2Url: string (nullable = true)\n",
      " |-- sources: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_df = spark_df.sample(fraction=0.1, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|         inCitations|        outCitations|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|b9c27ac1bb8b3ec9d...|                  []|                  []|\n",
      "|91cac7b12800a6238...|                  []|                  []|\n",
      "|412b10a92babf3509...|                  []|                  []|\n",
      "|0de1cecd2ed49812f...|[8ecaab2a03953fa5...|[cf5cdba6424524ee...|\n",
      "|1d9fee40f59bf00eb...|                  []|                  []|\n",
      "|5e33c40c33d9e4753...|                  []|                  []|\n",
      "|d477d10c48912335d...|                  []|                  []|\n",
      "|b87f8f3daaa92d3bc...|                  []|                  []|\n",
      "|598567aa0229a6b31...|[ecec08f5ca15e07d...|                  []|\n",
      "|b790f30e811445a68...|[2da3df885d1f032d...|                  []|\n",
      "|69a00b492e0ba6e25...|                  []|                  []|\n",
      "|6b75f9994d131b2ee...|[ac89cd3e34dbc788...|                  []|\n",
      "|9eda969296439bf4c...|                  []|[411f288349de96c0...|\n",
      "|47bee8ebe530dc4ff...|[e5951f7aa9eef9c4...|[3dc9587f4b564c90...|\n",
      "|79ac36d7f74cf8d83...|                  []|                  []|\n",
      "|e079322082ef30858...|                  []|                  []|\n",
      "|c5bf1145b594c518b...|                  []|[0742fa40bf9be455...|\n",
      "|bd72c9e0434214845...|                  []|                  []|\n",
      "|d125a6c2adbfa1932...|                  []|                  []|\n",
      "|4acf97f40b5e9b991...|                  []|                  []|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select('id', 'inCitations', 'outCitations').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>authors</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doi</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doiUrl</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entities</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldsOfStudy</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inCitations</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalName</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalPages</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalVolume</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outCitations</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paperAbstract</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdfUrls</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmid</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2PdfUrl</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2Url</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sources</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venue</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>192481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "authors             0\n",
       "doi                 0\n",
       "doiUrl              0\n",
       "entities            0\n",
       "fieldsOfStudy       0\n",
       "id                  0\n",
       "inCitations         0\n",
       "journalName         0\n",
       "journalPages        0\n",
       "journalVolume       0\n",
       "outCitations        0\n",
       "paperAbstract       0\n",
       "pdfUrls             0\n",
       "pmid                0\n",
       "s2PdfUrl            0\n",
       "s2Url               0\n",
       "sources             0\n",
       "title               0\n",
       "venue               0\n",
       "year           192481"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "spark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = spark_df.select('id', 'title', 'year', 'fieldsOfStudy', 'paperAbstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Getting the inCitations id -> (cited) -> article id \n",
    "edges = spark_df.select(explode('inCitations').alias('src'), spark_df.id.alias('dst')).withColumn('type', lit('cited'))\n",
    "\n",
    "#Getting the article id -> (cited) -> outCitations id\n",
    "edges2 = spark_df.select(spark_df.id.alias('src'), explode('outCitations').alias('dst')).withColumn('type', lit('cited'))\n",
    "\n",
    "#Union of these two \n",
    "edges_total = edges.union(edges2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+-----+\n",
      "|src                                     |dst                                     |type |\n",
      "+----------------------------------------+----------------------------------------+-----+\n",
      "|8ecaab2a03953fa5cf08ab5db2bb49c16d90527d|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "|8ea68d48d5595730f082f625a2ad759f934411cf|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "|766a2b4d54b541c737f6a6ec81ad487b56dc83ed|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "|8479962401362c07f2d6cbff871a11cbd7a4913f|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "|a0ab0cf6c8fe0b1e50407576ba5b80f0f3f58476|0de1cecd2ed49812f3a55e4b78edf9ffd292618c|cited|\n",
      "+----------------------------------------+----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_total.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                  id|degree|\n",
      "+--------------------+------+\n",
      "|83730969c0686b1d1...| 38079|\n",
      "|abd1c342495432171...| 32441|\n",
      "|13d4c2f76a7c1a4d0...| 20769|\n",
      "|a512385be058b1e2e...| 17759|\n",
      "|a411f6a0e6473137a...| 17398|\n",
      "|87f40e6f3022adbc1...| 12909|\n",
      "|73679f1ba00de9e73...| 11435|\n",
      "|e6dd0e6cf076b1207...| 11408|\n",
      "|d2860a370a0386c57...| 10815|\n",
      "|6a17ebeeb80cd696b...|  9429|\n",
      "|6da2a9ffa23a59523...|  9200|\n",
      "|10d6778bc45aebcd5...|  9121|\n",
      "|490020c0d4fa1eb85...|  9047|\n",
      "|fc448a7db5a2fac24...|  8676|\n",
      "|54205667c1f65a320...|  8613|\n",
      "|a42d6065d0b1a31c5...|  8255|\n",
      "|cc90910b6e31fe44c...|  8237|\n",
      "|6889e5a22598521d8...|  7679|\n",
      "|0e68beebb4c7ccbd9...|  7507|\n",
      "|9f649b234f5ebf207...|  7350|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(vertices, edges_total)\n",
    "## Take a look at the DataFrames\n",
    "# g.vertices.show()\n",
    "# g.edges.show()\n",
    "## Check the number of edges of each vertex\n",
    "g.degrees.sort(\"degree\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### indegree = The number of edges directed into a vertex in a directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|inDegree|\n",
      "+--------------------+--------+\n",
      "|83730969c0686b1d1...|   38079|\n",
      "|abd1c342495432171...|   32420|\n",
      "|13d4c2f76a7c1a4d0...|   20763|\n",
      "|a512385be058b1e2e...|   17634|\n",
      "|a411f6a0e6473137a...|   17370|\n",
      "|87f40e6f3022adbc1...|   12898|\n",
      "|73679f1ba00de9e73...|   11428|\n",
      "|e6dd0e6cf076b1207...|   11276|\n",
      "|d2860a370a0386c57...|   10815|\n",
      "|6a17ebeeb80cd696b...|    9429|\n",
      "|6da2a9ffa23a59523...|    9197|\n",
      "|10d6778bc45aebcd5...|    9120|\n",
      "|490020c0d4fa1eb85...|    9028|\n",
      "|fc448a7db5a2fac24...|    8676|\n",
      "|54205667c1f65a320...|    8611|\n",
      "|cc90910b6e31fe44c...|    8231|\n",
      "|a42d6065d0b1a31c5...|    8182|\n",
      "|6889e5a22598521d8...|    7656|\n",
      "|0e68beebb4c7ccbd9...|    7486|\n",
      "|9f649b234f5ebf207...|    7350|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.inDegrees.filter(\"inDegree >= 10\").sort(\"inDegree\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indegree = g.inDegrees.filter(\"inDegree >= 10\").sort(\"inDegree\", ascending=False).select('id').take(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>fieldsOfStudy</th>\n",
       "      <th>paperAbstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83730969c0686b1d185bcca39f9b5743fa53ebc1</td>\n",
       "      <td>R: A language and environment for statistical ...</td>\n",
       "      <td>2014</td>\n",
       "      <td>[Computer Science]</td>\n",
       "      <td>Copyright (©) 1999–2012 R Foundation for Stati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  83730969c0686b1d185bcca39f9b5743fa53ebc1   \n",
       "\n",
       "                                               title  year  \\\n",
       "0  R: A language and environment for statistical ...  2014   \n",
       "\n",
       "        fieldsOfStudy                                      paperAbstract  \n",
       "0  [Computer Science]  Copyright (©) 1999–2012 R Foundation for Stati...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_node = g.vertices.filter('id == \"{}\"'.format(top_indegree)).toPandas()\n",
    "top_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating checkpoint directory in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('graphframes_cps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at PageRank scores of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.pageRank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o184.run.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 18.0 failed 4 times, most recent failure: Lost task 14.3 in stage 18.0 (TID 3298, hd05.rcc.local, executor 303): java.lang.ClassNotFoundException: org.graphframes.GraphFrame$$anonfun$5\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1868)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:88)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2178)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n\tat org.apache.spark.graphx.Pregel$.apply(Pregel.scala:140)\n\tat org.apache.spark.graphx.lib.PageRank$.runUntilConvergenceWithOptions(PageRank.scala:355)\n\tat org.graphframes.lib.PageRank$.runUntilConvergence(PageRank.scala:152)\n\tat org.graphframes.lib.PageRank.run(PageRank.scala:102)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.graphframes.GraphFrame$$anonfun$5\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1868)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:88)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4cd2dd796711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpageRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresetProbability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/graphframes/graphframe.py\u001b[0m in \u001b[0;36mpageRank\u001b[0;34m(self, resetProbability, sourceId, maxIter, tol)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Exactly one of maxIter or tol should be set.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mjgf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_java_gf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sqlContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o184.run.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 18.0 failed 4 times, most recent failure: Lost task 14.3 in stage 18.0 (TID 3298, hd05.rcc.local, executor 303): java.lang.ClassNotFoundException: org.graphframes.GraphFrame$$anonfun$5\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1868)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:88)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2178)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat org.apache.spark.graphx.impl.VertexRDDImpl.count(VertexRDDImpl.scala:90)\n\tat org.apache.spark.graphx.Pregel$.apply(Pregel.scala:140)\n\tat org.apache.spark.graphx.lib.PageRank$.runUntilConvergenceWithOptions(PageRank.scala:355)\n\tat org.graphframes.lib.PageRank$.runUntilConvergence(PageRank.scala:152)\n\tat org.graphframes.lib.PageRank.run(PageRank.scala:102)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.graphframes.GraphFrame$$anonfun$5\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1868)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:88)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "pr = g.pageRank(resetProbability=0.15, tol=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.vertices)\n",
    "## look at the pagerank score for every vertex\n",
    "results.vertices.show()\n",
    "## look at the weight of every edge\n",
    "results.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
