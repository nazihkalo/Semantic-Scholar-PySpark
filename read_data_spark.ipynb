{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from HDFS\n",
    "\n",
    "Datasource: Semantic Scholar Open Research Corpus\n",
    "\n",
    "Description: Semantic Scholar's records for research papers published in all fields provided as an easy-to-use JSON archive.\n",
    "\n",
    "### Attribute Definitions\n",
    "1. id  string = S2 generated research paper ID.\n",
    "- title  string = Research paper title.\n",
    "- paperAbstract  string = Extracted abstract of the paper.\n",
    "- entities  list = Extracted list of relevant entities or topics.\n",
    "- s2Url  string = URL to S2 research paper details page.\n",
    "- s2PdfUrl  string = URL to PDF on S2 if available.\n",
    "- pdfUrls  list = URLs related to this PDF scraped from the web.\n",
    "- authors  list = List of authors with an S2 generated author ID and name.\n",
    "- inCitations  list = List of S2 paper IDs which cited this paper.\n",
    "- outCitations  list = List of S2 paper IDs which this paper cited\n",
    "- year  int = Year this paper was published as integer.\n",
    "- venue  string = Extracted publication venue for this paper.\n",
    "- journalName  string = Name of the journal that published this paper.\n",
    "- journalVolume  string = The volume of the journal where this paper was published.\n",
    "- journalPages  string = The pages of the journal where this paper was published.\n",
    "- sources  list = Identifies papers sourced from DBLP or Medline.\n",
    "- doi  string = Digital Object Identifier registered at doi.org.\n",
    "- doiUrl  string = DOI link for registered objects.\n",
    "- pmid  string = Unique identifier used by PubMed.\n",
    "- fieldsOfStudy  list = Zero or more fields of study this paper addresses.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.enableHiveSupport().appName('ReadWriteData').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MKL_NUM_THREADS': '1', 'OPENBLAS_NUM_THREADS': '1', 'PYTHONHASHSEED': '0'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.json('big_data_project/s2-corpus-000.txt')\n",
    "spark_df2 = spark.read.json('big_data_project/s2-corpus-001.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ids: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- doiUrl: string (nullable = true)\n",
      " |-- entities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fieldsOfStudy: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- journalName: string (nullable = true)\n",
      " |-- journalPages: string (nullable = true)\n",
      " |-- journalVolume: string (nullable = true)\n",
      " |-- outCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- paperAbstract: string (nullable = true)\n",
      " |-- pdfUrls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- s2PdfUrl: string (nullable = true)\n",
      " |-- s2Url: string (nullable = true)\n",
      " |-- sources: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ids: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- doiUrl: string (nullable = true)\n",
      " |-- entities: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fieldsOfStudy: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- journalName: string (nullable = true)\n",
      " |-- journalPages: string (nullable = true)\n",
      " |-- journalVolume: string (nullable = true)\n",
      " |-- outCitations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- paperAbstract: string (nullable = true)\n",
      " |-- pdfUrls: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- s2PdfUrl: string (nullable = true)\n",
      " |-- s2Url: string (nullable = true)\n",
      " |-- sources: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5955151"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df2 = spark_df2.drop(\"_corrupt_record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking that columns & datatypes are equal for both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the # of columns equal? ==> True\n",
      "Are the dtypes of columns equal? ==> True\n"
     ]
    }
   ],
   "source": [
    "print('Are the # of columns equal? ==>', len(spark_df2.columns) == len(spark_df.columns))\n",
    "\n",
    "print('Are the dtypes of columns equal? ==>', (spark_df2.dtypes) == (spark_df.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows expected after union =  6955022\n"
     ]
    }
   ],
   "source": [
    "print('Total number of rows expected after union = ',spark_df.count() + spark_df2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df3 = spark_df.union(spark_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6955022"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6820367"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df3.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1831"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df3.select('fieldsOfStudy').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|       fieldsOfStudy|  count|\n",
      "+--------------------+-------+\n",
      "|          [Medicine]|1266282|\n",
      "|                  []| 907093|\n",
      "|         [Chemistry]| 468369|\n",
      "|  [Computer Science]| 429008|\n",
      "|       [Engineering]| 378788|\n",
      "|           [Biology]| 345616|\n",
      "|           [Physics]| 325009|\n",
      "| [Materials Science]| 262046|\n",
      "| [Biology, Medicine]| 234569|\n",
      "|           [History]| 229989|\n",
      "|        [Psychology]| 216940|\n",
      "|               [Art]| 206989|\n",
      "|       [Mathematics]| 189919|\n",
      "| [Political Science]| 189145|\n",
      "|         [Sociology]| 174594|\n",
      "|         [Geography]| 173749|\n",
      "|         [Economics]| 142621|\n",
      "|          [Business]| 135709|\n",
      "|           [Geology]| 117060|\n",
      "|[Chemistry, Medic...| 114251|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df3.groupby('fieldsOfStudy').count().sort(\"count\").orderBy([\"count\"], ascending=[0]).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df3.select('year').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating column that counts the inCitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf #user defined function\n",
    "from pyspark.sql.types import * #Import types == IntegerType, StringType etc.\n",
    "\n",
    "length = udf(lambda listt: len(listt), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df3 = spark_df3.withColumn('inCitations_count', length(spark_df3['inCitations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|inCitations_count|         inCitations|\n",
      "+-----------------+--------------------+\n",
      "|                6|[de4369814785d2f3...|\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|               30|[ab5c10646896d921...|\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|                0|                  []|\n",
      "|              129|[c4e7a25f671de435...|\n",
      "|                6|[defe6d4c1d146183...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df3.select('inCitations_count', 'inCitations').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating column that for length of abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       paperAbstract|\n",
      "+--------------------+\n",
      "|The main objectiv...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df3.select('paperAbstract').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construction 2\n",
    "# from spacy.lang.en import English\n",
    "# nlp = English()\n",
    "# # Create a Tokenizer with the default settings for English\n",
    "# # including punctuation rules and exceptions\n",
    "# tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = udf(lambda string: len(nltk.word_tokenize(string)), IntegerType())\n",
    "\n",
    "spark_df3 = spark_df3.withColumn('paperAbstract_wcount', word_count(spark_df3['paperAbstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|paperAbstract_wcount|       paperAbstract|\n",
      "+--------------------+--------------------+\n",
      "|                 405|The main objectiv...|\n",
      "|                  57|This article refl...|\n",
      "|                 183|In this work, kee...|\n",
      "|                  29|Resumen pt: O obj...|\n",
      "|                 192|The Financial sec...|\n",
      "|                   0|                    |\n",
      "|                   0|                    |\n",
      "|                   0|                    |\n",
      "|                 142|The progress in m...|\n",
      "|                 159|Abstract This stu...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df3.select('paperAbstract_wcount', 'paperAbstract').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------+\n",
      "|title_wcount|                                title|\n",
      "+------------+-------------------------------------+\n",
      "|          19|                 Relationship betw...|\n",
      "|          15|                 Aplicación y crít...|\n",
      "|          11|                 On decomposition ...|\n",
      "|          11|                 Reflexões Sobre O...|\n",
      "|           3|                 Banking Sector Re...|\n",
      "|          18|                 [identification, ...|\n",
      "|           9|                 SARS control: Fir...|\n",
      "|           2|日本語教育における助数詞の扱いの問...|\n",
      "|          10|                 Continuous separa...|\n",
      "|          13|                 Aqueous phase ref...|\n",
      "+------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df3 = spark_df3.withColumn('title_wcount', word_count(spark_df3['title']))\n",
    "spark_df3.select('title_wcount', 'title').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing some dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the list + string into string\n",
    "spark_df4 = spark_df3.withColumn(\"authors\", spark_df3[\"authors\"].cast(StringType()))\n",
    "spark_df4 = spark_df4.withColumn(\"entities\", spark_df4[\"entities\"].cast(StringType()))\n",
    "spark_df4 = spark_df4.withColumn(\"fieldsOfStudy\", spark_df4[\"fieldsOfStudy\"].cast(StringType()))\n",
    "spark_df4 = spark_df4.withColumn(\"inCitations\", spark_df4[\"inCitations\"].cast(StringType()))\n",
    "spark_df4 = spark_df4.withColumn(\"outCitations\", spark_df4[\"outCitations\"].cast(StringType()))\n",
    "spark_df4 = spark_df4.withColumn(\"pdfUrls\", spark_df4[\"pdfUrls\"].cast(StringType()))\n",
    "spark_df4 = spark_df4.withColumn(\"sources\", spark_df4[\"sources\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#split authors into authors/id into multiple columns \n",
    "split_col = F.split(spark_df4['authors'], ',')\n",
    "\n",
    "spark_df4 = spark_df4.withColumn('ID', split_col.getItem(0))\n",
    "spark_df4 = spark_df4.withColumn('authors', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             authors|           ID|\n",
      "+--------------------+-------------+\n",
      "|     Frederik Accoe]| [[[11348185]|\n",
      "| Enrique Cuna Pér...| [[[50657823]|\n",
      "|          N. Shiiba]| [[[14019388]|\n",
      "| Ariane Fazzolo S...| [[[14573970]|\n",
      "| Obida Gobna Wafu...|[[[119356007]|\n",
      "|         A. Pittera]| [[[11465179]|\n",
      "|       Carrie Lock]]|[[[145371798]|\n",
      "|         幸子 北川]]|[[[135803841]|\n",
      "|    Andreas Lenshof]|  [[[3435821]|\n",
      "|    Toshiaki Nozawa]| [[[92141798]|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df4.select('authors', 'ID').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('authors', 'string'),\n",
       " ('doi', 'string'),\n",
       " ('doiUrl', 'string'),\n",
       " ('entities', 'string'),\n",
       " ('fieldsOfStudy', 'string'),\n",
       " ('ID', 'string'),\n",
       " ('inCitations', 'string'),\n",
       " ('journalName', 'string'),\n",
       " ('journalPages', 'string'),\n",
       " ('journalVolume', 'string'),\n",
       " ('outCitations', 'string'),\n",
       " ('paperAbstract', 'string'),\n",
       " ('pdfUrls', 'string'),\n",
       " ('pmid', 'string'),\n",
       " ('s2PdfUrl', 'string'),\n",
       " ('s2Url', 'string'),\n",
       " ('sources', 'string'),\n",
       " ('title', 'string'),\n",
       " ('venue', 'string'),\n",
       " ('year', 'bigint'),\n",
       " ('inCitations_count', 'int'),\n",
       " ('paperAbstract_wcount', 'int'),\n",
       " ('title_wcount', 'int')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|       fieldsOfStudy|  count|\n",
      "+--------------------+-------+\n",
      "|          [Medicine]|1266282|\n",
      "|                  []| 907093|\n",
      "|         [Chemistry]| 468369|\n",
      "|  [Computer Science]| 429008|\n",
      "|       [Engineering]| 378788|\n",
      "|           [Biology]| 345616|\n",
      "|           [Physics]| 325009|\n",
      "| [Materials Science]| 262046|\n",
      "| [Biology, Medicine]| 234569|\n",
      "|           [History]| 229989|\n",
      "|        [Psychology]| 216940|\n",
      "|               [Art]| 206989|\n",
      "|       [Mathematics]| 189919|\n",
      "| [Political Science]| 189145|\n",
      "|         [Sociology]| 174594|\n",
      "|         [Geography]| 173749|\n",
      "|         [Economics]| 142621|\n",
      "|          [Business]| 135709|\n",
      "|           [Geology]| 117060|\n",
      "|[Chemistry, Medic...| 114251|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fields_count = spark_df4.groupby('fieldsOfStudy').count().sort('count', ascending = False)\n",
    "fields_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#convert Pyspark dataframe to Pandas dataframe for plotting\n",
    "pdf = fields_count.toPandas()\n",
    "pdf[:20].plot(kind= 'bar', x='fieldsOfStudy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 85 in stage 29.0 failed 4 times, most recent failure: Lost task 85.3 in stage 29.0 (TID 1674, hd04.rcc.local, executor 317): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-0e6593b4a1c9>\", line 4, in <lambda>\nTypeError: object of type 'NoneType' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1993)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2146)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-0e6593b4a1c9>\", line 4, in <lambda>\nTypeError: object of type 'NoneType' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1993)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-68fc29c31fb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Doing the heavy lifting in Spark. We could leverage the `histogram` function from the RDD api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minCitations_count_histogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inCitations_count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(self, buckets)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m                 \u001b[0mminv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\" empty \"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 85 in stage 29.0 failed 4 times, most recent failure: Lost task 85.3 in stage 29.0 (TID 1674, hd04.rcc.local, executor 317): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-0e6593b4a1c9>\", line 4, in <lambda>\nTypeError: object of type 'NoneType' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1993)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2146)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-0e6593b4a1c9>\", line 4, in <lambda>\nTypeError: object of type 'NoneType' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:557)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:345)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1993)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:194)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Doing the heavy lifting in Spark. We could leverage the `histogram` function from the RDD api\n",
    "\n",
    "inCitations_count_histogram = spark_df4.select('inCitations_count').rdd.flatMap(lambda x: x).histogram(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Computed Histogram into a Pandas Dataframe for plotting\n",
    "pd.DataFrame(\n",
    "    list(zip(*inCitations_count_histogram)), \n",
    "    columns=['bin', 'frequency']\n",
    ").set_index(\n",
    "    'bin'\n",
    ").plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o547.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 85 in stage 31.0 failed 4 times, most recent failure: Lost task 85.3 in stage 31.0 (TID 1852, hd03.rcc.local, executor 561): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-0e6593b4a1c9>\", line 4, in <lambda>\nTypeError: object of type 'NoneType' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1442)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-0e6593b4a1c9>\", line 4, in <lambda>\nTypeError: object of type 'NoneType' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1442)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0e64fa1714a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_df4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_papers.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o547.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 85 in stage 31.0 failed 4 times, most recent failure: Lost task 85.3 in stage 31.0 (TID 1852, hd03.rcc.local, executor 561): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-0e6593b4a1c9>\", line 4, in <lambda>\nTypeError: object of type 'NoneType' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1442)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/cloudera/parcels/CDH/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-0e6593b4a1c9>\", line 4, in <lambda>\nTypeError: object of type 'NoneType' has no len()\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:624)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1442)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\n"
     ]
    }
   ],
   "source": [
    "spark_df4.write.format('parquet').save('all_papers.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls all_papers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hive_context = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| databaseName|\n",
      "+-------------+\n",
      "|     aamorris|\n",
      "|    ababikova|\n",
      "|          abb|\n",
      "|      abertin|\n",
      "| abhishekchat|\n",
      "|      achenad|\n",
      "|  adeshghadge|\n",
      "|      adetola|\n",
      "| adhamsuliman|\n",
      "|   aditilakra|\n",
      "|       aghose|\n",
      "|       ahphan|\n",
      "|      airline|\n",
      "|       alphan|\n",
      "|       aluong|\n",
      "|alvinharyanto|\n",
      "|       amant3|\n",
      "|    amiparikh|\n",
      "|amoliterno945|\n",
      "|     andmoral|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show databaes\n",
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
